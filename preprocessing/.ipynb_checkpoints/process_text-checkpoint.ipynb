{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bfd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Prétraitement: Tokenisation, nettoyage et normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79990de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hasna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hasna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chargement du dataset depuis ../data/data_clean.csv...\n",
      " Prétraitement du texte (TI)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194938/194938 [00:13<00:00, 14105.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sauvegarde du fichier nettoyé dans ../data/preprocessed_data.csv...\n",
      " Prétraitement terminé avec succès !\n",
      " Fichier sauvegardé : ../data/preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Téléchargement des ressources NLTK si nécessaire ---\n",
    "try:\n",
    "    _ = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "# --- Initialisation des outils ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# --- FONCTION DE PRÉTRAITEMENT ---\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Nettoyage complet du texte : minuscules, suppression des caractères spéciaux, stopwords et lemmatisation.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "\n",
    "    # Minuscule\n",
    "    text = text.lower()\n",
    "\n",
    "    # Suppression des caractères spéciaux, chiffres, ponctuation\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "\n",
    "    # Suppression des espaces multiples\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Tokenisation simple\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Suppression des stopwords et des mots trop courts\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "\n",
    "    # Lemmatisation\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    # Reconstruction du texte propre\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    INPUT_PATH = \"../data/data_clean.csv\"  \n",
    "    OUTPUT_PATH = \"../data/preprocessed_data.csv\"\n",
    "\n",
    "    print(f\" Chargement du dataset depuis {INPUT_PATH}...\")\n",
    "    df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "    # Choisir la colonne texte : 'TI' ou 'abstract'\n",
    "    text_column = 'TI' if 'TI' in df.columns else 'abstract'\n",
    "\n",
    "    print(f\" Prétraitement du texte ({text_column})...\")\n",
    "    tqdm.pandas()\n",
    "    df['clean_text'] = df[text_column].progress_apply(preprocess_text)\n",
    "\n",
    "    # Supprimer la colonne label si elle existe (au cas où)\n",
    "    if 'label' in df.columns:\n",
    "        df = df.drop(columns=['label'])\n",
    "\n",
    "    print(f\" Sauvegarde du fichier nettoyé dans {OUTPUT_PATH}...\")\n",
    "    df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "    print(\" Prétraitement terminé avec succès !\")\n",
    "    print(f\" Fichier sauvegardé : {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94714c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2018, 2018, 2018, ..., 2019, 2019, 2019], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\hasna\\Desktop\\Master\\S3\\NLP\\Projet-NLP\\data\\preprocessed_data.csv\")\n",
    "df['PY'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adbf4dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    194938.000000\n",
      "mean       2013.708056\n",
      "std           5.940576\n",
      "min        1961.000000\n",
      "25%        2011.000000\n",
      "50%        2015.000000\n",
      "75%        2018.000000\n",
      "max        2020.000000\n",
      "Name: PY, dtype: float64\n",
      "[1961, 1967, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976]  ...  [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"PY\"].describe())\n",
    "print(sorted(df[\"PY\"].unique())[:10], \" ... \", sorted(df[\"PY\"].unique())[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8347ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Chemin vers le dataset complet\n",
    "DATA_PATH = r\"C:\\Users\\hasna\\Desktop\\Master\\S3\\NLP\\Projet-NLP\\data\\preprocessed_data.csv\"\n",
    "\n",
    "# Chargement du dataset complet\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\" Dataset complet chargé : {df.shape[0]} lignes\")\n",
    "\n",
    "# Sélection aléatoire ou les premières 10 000 lignes\n",
    "df_small = df.head(30000)  # ou df.sample(n=10000, random_state=42) pour un échantillon aléatoire\n",
    "\n",
    "print(f\" Dataset réduit : {df_small.shape[0]} lignes\")\n",
    "\n",
    "# Sauvegarde du dataset réduit\n",
    "SAVE_PATH = r\"C:\\Users\\hasna\\Desktop\\Master\\S3\\NLP\\Projet-NLP\\data\\preprocessed_data_small.csv\"\n",
    "df_small.to_csv(SAVE_PATH, index=False)\n",
    "print(f\" Dataset réduit sauvegardé dans : {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2e9120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PY</th>\n",
       "      <th>id</th>\n",
       "      <th>eid</th>\n",
       "      <th>TI</th>\n",
       "      <th>author</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>85058550296</td>\n",
       "      <td>2-s2.0-85058550296</td>\n",
       "      <td>knowledge capture and reuse through expert’s a...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>knowledge capture reuse expert activity monito...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>85058873457</td>\n",
       "      <td>2-s2.0-85058873457</td>\n",
       "      <td>an approach of energy resources control system...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>approach energy resource control system design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>85058993388</td>\n",
       "      <td>2-s2.0-85058993388</td>\n",
       "      <td>som-like neural network and differential evolu...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>som like neural network differential evolution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>85059318297</td>\n",
       "      <td>2-s2.0-85059318297</td>\n",
       "      <td>drug discovery and drug marketing with the cri...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>drug discovery drug marketing critical role mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>85059937495</td>\n",
       "      <td>2-s2.0-85059937495</td>\n",
       "      <td>towards a natural language compiler</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>towards natural language compiler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018</td>\n",
       "      <td>85044445280</td>\n",
       "      <td>2-s2.0-85044445280</td>\n",
       "      <td>spontaneous emergence of programs from “primor...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>spontaneous emergence program primordial soup ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018</td>\n",
       "      <td>85044714101</td>\n",
       "      <td>2-s2.0-85044714101</td>\n",
       "      <td>towards high-performance python</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>towards high performance python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018</td>\n",
       "      <td>85045383599</td>\n",
       "      <td>2-s2.0-85045383599</td>\n",
       "      <td>the online set aggregation problem</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>online set aggregation problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018</td>\n",
       "      <td>85045426562</td>\n",
       "      <td>2-s2.0-85045426562</td>\n",
       "      <td>practical, anonymous, and publicly linkable un...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>practical anonymous publicly linkable universa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018</td>\n",
       "      <td>85046626398</td>\n",
       "      <td>2-s2.0-85046626398</td>\n",
       "      <td>a distributional semantics model for idiom det...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>distributional semantics model idiom detection...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PY           id                 eid  \\\n",
       "0  2018  85058550296  2-s2.0-85058550296   \n",
       "1  2018  85058873457  2-s2.0-85058873457   \n",
       "2  2018  85058993388  2-s2.0-85058993388   \n",
       "3  2018  85059318297  2-s2.0-85059318297   \n",
       "4  2018  85059937495  2-s2.0-85059937495   \n",
       "5  2018  85044445280  2-s2.0-85044445280   \n",
       "6  2018  85044714101  2-s2.0-85044714101   \n",
       "7  2018  85045383599  2-s2.0-85045383599   \n",
       "8  2018  85045426562  2-s2.0-85045426562   \n",
       "9  2018  85046626398  2-s2.0-85046626398   \n",
       "\n",
       "                                                  TI  \\\n",
       "0  knowledge capture and reuse through expert’s a...   \n",
       "1  an approach of energy resources control system...   \n",
       "2  som-like neural network and differential evolu...   \n",
       "3  drug discovery and drug marketing with the cri...   \n",
       "4                towards a natural language compiler   \n",
       "5  spontaneous emergence of programs from “primor...   \n",
       "6                    towards high-performance python   \n",
       "7                 the online set aggregation problem   \n",
       "8  practical, anonymous, and publicly linkable un...   \n",
       "9  a distributional semantics model for idiom det...   \n",
       "\n",
       "                                              author  \\\n",
       "0  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "1  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "2  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "3  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "4  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "5  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "6  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "7  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "8  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "9  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  knowledge capture reuse expert activity monito...  \n",
       "1     approach energy resource control system design  \n",
       "2  som like neural network differential evolution...  \n",
       "3  drug discovery drug marketing critical role mo...  \n",
       "4                  towards natural language compiler  \n",
       "5  spontaneous emergence program primordial soup ...  \n",
       "6                    towards high performance python  \n",
       "7                     online set aggregation problem  \n",
       "8  practical anonymous publicly linkable universa...  \n",
       "9  distributional semantics model idiom detection...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24839790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
