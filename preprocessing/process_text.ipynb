{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bfd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Prétraitement: Tokenisation, nettoyage et normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79990de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hasna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hasna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chargement du dataset depuis ../data/data_clean.csv...\n",
      " Prétraitement du texte (TI)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194938/194938 [00:13<00:00, 14105.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sauvegarde du fichier nettoyé dans ../data/preprocessed_data.csv...\n",
      " Prétraitement terminé avec succès !\n",
      " Fichier sauvegardé : ../data/preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Téléchargement des ressources NLTK si nécessaire ---\n",
    "try:\n",
    "    _ = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "# --- Initialisation des outils ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# --- FONCTION DE PRÉTRAITEMENT ---\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Nettoyage complet du texte : minuscules, suppression des caractères spéciaux, stopwords et lemmatisation.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "\n",
    "    # Minuscule\n",
    "    text = text.lower()\n",
    "\n",
    "    # Suppression des caractères spéciaux, chiffres, ponctuation\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "\n",
    "    # Suppression des espaces multiples\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Tokenisation simple\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Suppression des stopwords et des mots trop courts\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "\n",
    "    # Lemmatisation\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    # Reconstruction du texte propre\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    INPUT_PATH = \"../data/data_clean.csv\"  \n",
    "    OUTPUT_PATH = \"../data/preprocessed_data.csv\"\n",
    "\n",
    "    print(f\" Chargement du dataset depuis {INPUT_PATH}...\")\n",
    "    df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "    # Choisir la colonne texte : 'TI' ou 'abstract'\n",
    "    text_column = 'TI' if 'TI' in df.columns else 'abstract'\n",
    "\n",
    "    print(f\" Prétraitement du texte ({text_column})...\")\n",
    "    tqdm.pandas()\n",
    "    df['clean_text'] = df[text_column].progress_apply(preprocess_text)\n",
    "\n",
    "    # Supprimer la colonne label si elle existe (au cas où)\n",
    "    if 'label' in df.columns:\n",
    "        df = df.drop(columns=['label'])\n",
    "\n",
    "    print(f\" Sauvegarde du fichier nettoyé dans {OUTPUT_PATH}...\")\n",
    "    df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "    print(\" Prétraitement terminé avec succès !\")\n",
    "    print(f\" Fichier sauvegardé : {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94714c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2018, 2018, 2018, ..., 2019, 2019, 2019], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\hasna\\Desktop\\Master\\S3\\NLP\\Projet-NLP\\data\\preprocessed_data.csv\")\n",
    "df['PY'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adbf4dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    194938.000000\n",
      "mean       2013.708056\n",
      "std           5.940576\n",
      "min        1961.000000\n",
      "25%        2011.000000\n",
      "50%        2015.000000\n",
      "75%        2018.000000\n",
      "max        2020.000000\n",
      "Name: PY, dtype: float64\n",
      "[1961, 1967, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976]  ...  [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"PY\"].describe())\n",
    "print(sorted(df[\"PY\"].unique())[:10], \" ... \", sorted(df[\"PY\"].unique())[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8347ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Chemin vers le dataset complet\n",
    "DATA_PATH = r\"C:\\Users\\hasna\\Desktop\\Master\\S3\\NLP\\Projet-NLP\\data\\preprocessed_data.csv\"\n",
    "\n",
    "# Chargement du dataset complet\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\" Dataset complet chargé : {df.shape[0]} lignes\")\n",
    "\n",
    "# Sélection aléatoire ou les premières 10 000 lignes\n",
    "df_small = df.head(30000)  # ou df.sample(n=10000, random_state=42) pour un échantillon aléatoire\n",
    "\n",
    "print(f\" Dataset réduit : {df_small.shape[0]} lignes\")\n",
    "\n",
    "# Sauvegarde du dataset réduit\n",
    "SAVE_PATH = r\"C:\\Users\\hasna\\Desktop\\Master\\S3\\NLP\\Projet-NLP\\data\\preprocessed_data_small.csv\"\n",
    "df_small.to_csv(SAVE_PATH, index=False)\n",
    "print(f\" Dataset réduit sauvegardé dans : {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2e9120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PY</th>\n",
       "      <th>id</th>\n",
       "      <th>eid</th>\n",
       "      <th>TI</th>\n",
       "      <th>author</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>85058550296</td>\n",
       "      <td>2-s2.0-85058550296</td>\n",
       "      <td>knowledge capture and reuse through expert’s a...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>knowledge capture reuse expert activity monito...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>85058873457</td>\n",
       "      <td>2-s2.0-85058873457</td>\n",
       "      <td>an approach of energy resources control system...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>approach energy resource control system design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>85058993388</td>\n",
       "      <td>2-s2.0-85058993388</td>\n",
       "      <td>som-like neural network and differential evolu...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>som like neural network differential evolution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>85059318297</td>\n",
       "      <td>2-s2.0-85059318297</td>\n",
       "      <td>drug discovery and drug marketing with the cri...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>drug discovery drug marketing critical role mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>85059937495</td>\n",
       "      <td>2-s2.0-85059937495</td>\n",
       "      <td>towards a natural language compiler</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>towards natural language compiler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018</td>\n",
       "      <td>85044445280</td>\n",
       "      <td>2-s2.0-85044445280</td>\n",
       "      <td>spontaneous emergence of programs from “primor...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>spontaneous emergence program primordial soup ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018</td>\n",
       "      <td>85044714101</td>\n",
       "      <td>2-s2.0-85044714101</td>\n",
       "      <td>towards high-performance python</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>towards high performance python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018</td>\n",
       "      <td>85045383599</td>\n",
       "      <td>2-s2.0-85045383599</td>\n",
       "      <td>the online set aggregation problem</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>online set aggregation problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018</td>\n",
       "      <td>85045426562</td>\n",
       "      <td>2-s2.0-85045426562</td>\n",
       "      <td>practical, anonymous, and publicly linkable un...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>practical anonymous publicly linkable universa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018</td>\n",
       "      <td>85046626398</td>\n",
       "      <td>2-s2.0-85046626398</td>\n",
       "      <td>a distributional semantics model for idiom det...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>distributional semantics model idiom detection...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PY           id                 eid  \\\n",
       "0  2018  85058550296  2-s2.0-85058550296   \n",
       "1  2018  85058873457  2-s2.0-85058873457   \n",
       "2  2018  85058993388  2-s2.0-85058993388   \n",
       "3  2018  85059318297  2-s2.0-85059318297   \n",
       "4  2018  85059937495  2-s2.0-85059937495   \n",
       "5  2018  85044445280  2-s2.0-85044445280   \n",
       "6  2018  85044714101  2-s2.0-85044714101   \n",
       "7  2018  85045383599  2-s2.0-85045383599   \n",
       "8  2018  85045426562  2-s2.0-85045426562   \n",
       "9  2018  85046626398  2-s2.0-85046626398   \n",
       "\n",
       "                                                  TI  \\\n",
       "0  knowledge capture and reuse through expert’s a...   \n",
       "1  an approach of energy resources control system...   \n",
       "2  som-like neural network and differential evolu...   \n",
       "3  drug discovery and drug marketing with the cri...   \n",
       "4                towards a natural language compiler   \n",
       "5  spontaneous emergence of programs from “primor...   \n",
       "6                    towards high-performance python   \n",
       "7                 the online set aggregation problem   \n",
       "8  practical, anonymous, and publicly linkable un...   \n",
       "9  a distributional semantics model for idiom det...   \n",
       "\n",
       "                                              author  \\\n",
       "0  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "1  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "2  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "3  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "4  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "5  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "6  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "7  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "8  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "9  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  knowledge capture reuse expert activity monito...  \n",
       "1     approach energy resource control system design  \n",
       "2  som like neural network differential evolution...  \n",
       "3  drug discovery drug marketing critical role mo...  \n",
       "4                  towards natural language compiler  \n",
       "5  spontaneous emergence program primordial soup ...  \n",
       "6                    towards high performance python  \n",
       "7                     online set aggregation problem  \n",
       "8  practical anonymous publicly linkable universa...  \n",
       "9  distributional semantics model idiom detection...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24839790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset complet chargé : 194938 lignes\n",
      " Reste du dataset : 164938 lignes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PY</th>\n",
       "      <th>id</th>\n",
       "      <th>eid</th>\n",
       "      <th>TI</th>\n",
       "      <th>author</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>85026204687</td>\n",
       "      <td>2-s2.0-85026204687</td>\n",
       "      <td>an approach using the design science research ...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>approach using design science research develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>85026192956</td>\n",
       "      <td>2-s2.0-85026192956</td>\n",
       "      <td>group matching for peer mentorship in small gr...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>group matching peer mentorship small group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>85025176830</td>\n",
       "      <td>2-s2.0-85025176830</td>\n",
       "      <td>an innovative hybrid model based on data pre-p...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>innovative hybrid model based data pre process...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>85025169375</td>\n",
       "      <td>2-s2.0-85025169375</td>\n",
       "      <td>ivhd: a robust linear-time and memory efficien...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>ivhd robust linear time memory efficient metho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>85025166493</td>\n",
       "      <td>2-s2.0-85025166493</td>\n",
       "      <td>self-adaptive uis: integrated model-driven dev...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>self adaptive uis integrated model driven deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017</td>\n",
       "      <td>85025144345</td>\n",
       "      <td>2-s2.0-85025144345</td>\n",
       "      <td>optimization for large-scale machine learning ...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>optimization large scale machine learning dist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017</td>\n",
       "      <td>85025124532</td>\n",
       "      <td>2-s2.0-85025124532</td>\n",
       "      <td>multi-view approach to parkinson’s disease qua...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>multi view approach parkinson disease quality ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017</td>\n",
       "      <td>85025124016</td>\n",
       "      <td>2-s2.0-85025124016</td>\n",
       "      <td>context-awareness and mobile hci: implications...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>context awareness mobile hci implication chall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017</td>\n",
       "      <td>85025123755</td>\n",
       "      <td>2-s2.0-85025123755</td>\n",
       "      <td>introducing a decision making framework to hel...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>introducing decision making framework help use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017</td>\n",
       "      <td>85025115857</td>\n",
       "      <td>2-s2.0-85025115857</td>\n",
       "      <td>a new approach to telecommunications network d...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>new approach telecommunication network design ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PY           id                 eid  \\\n",
       "0  2017  85026204687  2-s2.0-85026204687   \n",
       "1  2017  85026192956  2-s2.0-85026192956   \n",
       "2  2017  85025176830  2-s2.0-85025176830   \n",
       "3  2017  85025169375  2-s2.0-85025169375   \n",
       "4  2017  85025166493  2-s2.0-85025166493   \n",
       "5  2017  85025144345  2-s2.0-85025144345   \n",
       "6  2017  85025124532  2-s2.0-85025124532   \n",
       "7  2017  85025124016  2-s2.0-85025124016   \n",
       "8  2017  85025123755  2-s2.0-85025123755   \n",
       "9  2017  85025115857  2-s2.0-85025115857   \n",
       "\n",
       "                                                  TI  \\\n",
       "0  an approach using the design science research ...   \n",
       "1  group matching for peer mentorship in small gr...   \n",
       "2  an innovative hybrid model based on data pre-p...   \n",
       "3  ivhd: a robust linear-time and memory efficien...   \n",
       "4  self-adaptive uis: integrated model-driven dev...   \n",
       "5  optimization for large-scale machine learning ...   \n",
       "6  multi-view approach to parkinson’s disease qua...   \n",
       "7  context-awareness and mobile hci: implications...   \n",
       "8  introducing a decision making framework to hel...   \n",
       "9  a new approach to telecommunications network d...   \n",
       "\n",
       "                                              author  \\\n",
       "0  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "1  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "2  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "3  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "4  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "5  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "6  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "7  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "8  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "9  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  approach using design science research develop...  \n",
       "1         group matching peer mentorship small group  \n",
       "2  innovative hybrid model based data pre process...  \n",
       "3  ivhd robust linear time memory efficient metho...  \n",
       "4  self adaptive uis integrated model driven deve...  \n",
       "5  optimization large scale machine learning dist...  \n",
       "6  multi view approach parkinson disease quality ...  \n",
       "7  context awareness mobile hci implication chall...  \n",
       "8  introducing decision making framework help use...  \n",
       "9  new approach telecommunication network design ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Chemin vers le dataset complet\n",
    "DATA_PATH = r\"C:\\Users\\hasna\\Desktop\\Master\\S3\\NLP\\Projet-NLP\\data\\preprocessed_data.csv\"\n",
    "\n",
    "# Chargement du dataset complet\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\" Dataset complet chargé : {df.shape[0]} lignes\")\n",
    "\n",
    "# ---- Sélection des 30 000 premières lignes ----\n",
    "df_first_30k = df.head(30000)\n",
    "\n",
    "# ---- Reste du dataset : à partir de la ligne 30000 ----\n",
    "df_rest = df.iloc[30000:].reset_index(drop=True)\n",
    "print(f\" Reste du dataset : {df_rest.shape[0]} lignes\")\n",
    "\n",
    "df_rest.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0430732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
