{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d75c3cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\st-cpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "549923c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chargement du dataset...\n",
      " Dataset chargé : 30000 lignes, 6 colonnes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PY</th>\n",
       "      <th>id</th>\n",
       "      <th>eid</th>\n",
       "      <th>TI</th>\n",
       "      <th>author</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>85058550296</td>\n",
       "      <td>2-s2.0-85058550296</td>\n",
       "      <td>knowledge capture and reuse through expert’s a...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>knowledge capture reuse expert activity monito...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>85058873457</td>\n",
       "      <td>2-s2.0-85058873457</td>\n",
       "      <td>an approach of energy resources control system...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>approach energy resource control system design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>85058993388</td>\n",
       "      <td>2-s2.0-85058993388</td>\n",
       "      <td>som-like neural network and differential evolu...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>som like neural network differential evolution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>85059318297</td>\n",
       "      <td>2-s2.0-85059318297</td>\n",
       "      <td>drug discovery and drug marketing with the cri...</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>drug discovery drug marketing critical role mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>85059937495</td>\n",
       "      <td>2-s2.0-85059937495</td>\n",
       "      <td>towards a natural language compiler</td>\n",
       "      <td>[{'@_fa': 'true', '@seq': '1', 'author-url': '...</td>\n",
       "      <td>towards natural language compiler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PY           id                 eid  \\\n",
       "0  2018  85058550296  2-s2.0-85058550296   \n",
       "1  2018  85058873457  2-s2.0-85058873457   \n",
       "2  2018  85058993388  2-s2.0-85058993388   \n",
       "3  2018  85059318297  2-s2.0-85059318297   \n",
       "4  2018  85059937495  2-s2.0-85059937495   \n",
       "\n",
       "                                                  TI  \\\n",
       "0  knowledge capture and reuse through expert’s a...   \n",
       "1  an approach of energy resources control system...   \n",
       "2  som-like neural network and differential evolu...   \n",
       "3  drug discovery and drug marketing with the cri...   \n",
       "4                towards a natural language compiler   \n",
       "\n",
       "                                              author  \\\n",
       "0  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "1  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "2  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "3  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "4  [{'@_fa': 'true', '@seq': '1', 'author-url': '...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  knowledge capture reuse expert activity monito...  \n",
       "1     approach energy resource control system design  \n",
       "2  som like neural network differential evolution...  \n",
       "3  drug discovery drug marketing critical role mo...  \n",
       "4                  towards natural language compiler  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===  Chargement du dataset ===\n",
    "DATA_PATH = r\"C:\\Users\\hasna\\Desktop\\Master\\S3\\NLP\\Projet-NLP\\data\\preprocessed_data_small.csv\"\n",
    "\n",
    "print(\" Chargement du dataset...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\" Dataset chargé : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "\n",
    "# Vérification rapide\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c37609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Chargement du modèle MiniLM...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Chargement du modèle MiniLM...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;66;03m#importe un modèle pré-entraîné de la bibliothèque SentenceTransformers\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mSentenceTransformer\u001b[49m(model_name)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Modèle \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m chargé avec succès !\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# === Sélection du texte à encoder ===\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "# === Chargement du modèle MiniLM ===\n",
    "\n",
    "print(\"\\n Chargement du modèle MiniLM...\")\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2' #importe un modèle pré-entraîné de la bibliothèque SentenceTransformers\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\" Modèle '{model_name}' chargé avec succès !\")\n",
    "\n",
    "# === Sélection du texte à encoder ===\n",
    "\n",
    "text_col = 'clean_text' \n",
    "texts = df[text_col].astype(str).tolist() #convertis en une liste Python pour pouvoir la donner au modèle.\n",
    "\n",
    "# === Génération des embeddings ===\n",
    "\n",
    "#Ce que fait réellement model.encode(): Tokenisation :Chaque texte est découpé en sous-mots (tokens),Exemple : “Natural language processing” → [“Natural”, “language”, “process”, “##ing”].\n",
    "#Passage dans MiniLM :Ces tokens sont transformés en vecteurs de contexte grâce à un réseau de neurones profond (transformer à 6 couches).Chaque mot comprend le contexte des autres mots autour de lui.\n",
    "#Agrégation :Le modèle combine les vecteurs de mots pour produire un seul vecteur global représentant le sens complet de la phrase.\n",
    "#Normalisation :Le paramètre normalize_embeddings=True ajuste les vecteurs pour qu’ils aient tous la même norme (longueur 1), ce qui facilite le calcul de similarité cosinus.\n",
    "\n",
    "\n",
    "print(\"\\n Génération des embeddings\") \n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    batch_size=64,  #64* 6 couches= chaque vecteur a 384 dimensions\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True \n",
    ") \n",
    "\n",
    "print(f\" Embeddings générés : {embeddings.shape}\")\n",
    "\n",
    "# ===  Sauvegarde des embeddings ===\n",
    "\n",
    "SAVE_PATH = r\"C:\\Users\\hasna\\Desktop\\Master\\S3\\NLP\\Projet-NLP\\data\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "embeddings_file = os.path.join(SAVE_PATH, \"embeddings_small_minilm.npy\")\n",
    "np.save(embeddings_file, embeddings)\n",
    "\n",
    "print(f\" Embeddings sauvegardés dans : {embeddings_file}\")\n",
    "\n",
    "# === sauvegarder aussi le dataset enrichi ===\n",
    "\n",
    "df[\"embedding_index\"] = range(len(df))\n",
    "df.to_csv(os.path.join(SAVE_PATH, \"data_small_with_embeddings.csv\"), index=False)\n",
    "print(\" Dataset enrichi sauvegardé !\")\n",
    "\n",
    "# ===  Vérification finale ===\n",
    "\n",
    "print(\"\\n Vérification d’un exemple d’embedding :\") #affiche les 10 premières valeurs du vecteur du premier texte\n",
    "print(embeddings[0][:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ab244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c778e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (st-cpu)",
   "language": "python",
   "name": "st-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
